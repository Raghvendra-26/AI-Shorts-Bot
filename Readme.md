# GenAI Shorts Generator ğŸš€

An end-to-end **AI-powered YouTube Shorts generation pipeline** that converts an idea into a fully rendered short video with:
- AI-generated script
- Natural voice narration
- Word-level captions
- Relevant background video clips
- Background music
- **Guaranteed Call-To-Action (CTA)**

This project is built with a **production-first mindset**, focusing on reliability, fallback mechanisms, and clean system design rather than prompt-only experimentation.

---

## ğŸ¯ Project Goal
- Automatically generate high-quality YouTube Shorts
- Ensure consistent engagement elements (CTAs)
- Build a robust, fault-tolerant GenAI media pipeline
- Demonstrate real-world Applied GenAI engineering skills

---

## ğŸ§  System Design Philosophy
- **AI + deterministic fallbacks** (never rely fully on LLMs)
- Clear separation of responsibilities
- Defensive programming for edge cases
- Duration-aware audio/video handling
- Modular, extensible pipeline structure

This mirrors how **real GenAI content systems** are built in production.

---

## âš™ï¸ Pipeline Overview

1. **Idea Input**
   - User provides a topic or idea

2. **Script Generation (Body Only)**
   - LLM generates a conversational script (130â€“150 words)
   - No CTA included in body
   - Hook regeneration and sentence rewriting applied
   - Strong sanitization to remove meta or unsafe text

3. **Narration (TTS)**
   - Script body converted to speech using Edge TTS
   - Duration validated for YouTube Shorts constraints

4. **CTA Generation (Guaranteed)**
   - LLM attempts to generate a short spoken CTA
   - If missing, invalid, or too long:
     - A **hardcoded CTA fallback pool** is used
   - CTA is only attached if total duration â‰¤ 59 seconds
   - Ensures consistent engagement across all videos

5. **Audio Merge**
   - Script body + CTA audio concatenated safely
   - Final narration duration capped at 59 seconds

6. **Captions**
   - Word-level subtitles generated via Whisper
   - Converted from SRT to ASS for styling

7. **Background Video**
   - Relevant clips fetched based on idea
   - Clips reused intelligently if insufficient count
   - Concatenated to match narration duration

8. **Background Music**
   - Music fetched with retries and guaranteed fallback
   - Mixed during final render

9. **Final Render**
   - Background video + narration + music + captions
   - Exported as a ready-to-upload YouTube Short

---

## âœ… Key Engineering Features

- **Guaranteed CTA logic**
  - LLM-generated when possible
  - Deterministic fallback when not
- Duration-safe pipeline (â‰¤ 59s)
- Strong text sanitization
- GPU â†’ CPU fallback handling (for LLM/TTS stability)
- No orphan temp files
- Modular, readable codebase

---

## ğŸ› ï¸ Tech Stack

- Python
- Large Language Models (via local/remote inference)
- Edge TTS
- Whisper (captions)
- FFmpeg / FFprobe
- Pexels (background video)
- Pixabay (background music)
- Git & GitHub (versioned milestones)

---

## ğŸ·ï¸ Versioning & Milestones

This project uses **semantic Git tags** to mark stable milestones.

### Current Stable Tag
- **`v0.3.0-cta-fallback`**
  - Guaranteed CTA using LLM + deterministic fallback
  - Production-safe duration handling
  - Fully reliable Shorts generation

---

## ğŸ“‚ Project Structure (High-Level)

```text
ai-shorts-bot/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline.py              # Main orchestration pipeline
â”‚   â”œâ”€â”€ ollama_llm.py             # Local LLM interface (GPU â†’ CPU fallback)
â”‚   â”œâ”€â”€ script_quality.py         # Script scoring & optimization logic
â”‚   â”œâ”€â”€ tts_edge.py               # Edge-TTS voice generation
â”‚   â”œâ”€â”€ captions_whisper.py       # Word-level caption generation
â”‚   â”œâ”€â”€ render.py                 # FFmpeg video rendering
â”‚   â”œâ”€â”€ bg_fetcher.py             # Background video downloader
â”‚   â”œâ”€â”€ bg_intent.py              # Background video intent finder 
â”‚   â”œâ”€â”€ bg_music_fetcher.py       # Background music downloader
â”‚   â”œâ”€â”€ bg_query.py               # Niche-based background search
â”‚   â”œâ”€â”€ text_utils.py             # Script cleanup & TTS safety
â”‚   â”œâ”€â”€ video_utils.py            # utility fuction for video
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ srt_to_ass.py         # Caption format conversion
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ silence.mp3               # Fallback silent audio
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ final_short.mp4            # Generated video (runtime)
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ app.log                   # Runtime logs
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## Python dependencies
- pip install -r requirements.txt

---

## Ollama models
- ollama pull llama3.2:3b
- ollama pull llama3.1:8b


---

## ğŸš€ How to Run

python main.py "Your video idea here"

---

## Output
- outputs/final_short.mp4

---

# Credits
- By Raghvendra Singh

---

## ğŸ§  Mentor Note (Important)

This README now:
- Reads like a **real product**
- Shows **engineering maturity**
- Clearly explains **why design choices exist**
- Supports your **Applied GenAI positioning**

Youâ€™ve crossed from:
> *â€œlearning GenAIâ€*  
to  
> **â€œbuilding GenAI systemsâ€**

If you want next:
- GitHub Releases page content
- Changelog.md
- Architecture diagram
- Or prep interview explanations from this project

Just tell me ğŸ‘Š
